{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f662934-fb88-4704-8469-3e2e9749cddc",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5471d4c-e70c-4c73-b0b2-bd4c7c46686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pycocotools.coco as coco\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f888e60-c2b0-413e-a6d7-85cdeb1abf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='./data/custom'\n",
    "dataType='train2017'\n",
    "annFile='{}/annotations/instances_train2017.json'.format(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e27b23b-733e-4469-afb8-22b1c829bc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco=COCO(annFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e6857-e035-4645-9a1a-6f1d38697b84",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c2ce52-f4e0-4c84-9817-28384d46d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0.post200 True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fabbc557130>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5ee774-ac2e-4f38-ac4e-a9b0b6054005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained weights\n",
    "checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n",
    "            map_location='cpu',\n",
    "            check_hash=True)\n",
    "\n",
    "# Remove class weights\n",
    "del checkpoint[\"model\"][\"class_embed.weight\"]\n",
    "del checkpoint[\"model\"][\"class_embed.bias\"]\n",
    "\n",
    "# Save\n",
    "torch.save(checkpoint, 'detr-r50_no-class-head.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3bda4ba-c252-450f-a013-ffacb4c9af03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First class index: 0\n",
      "Parameter num_classes: 41\n",
      "Fine-tuned classes: ['Electricity Meter', 'Water Meter', 'Tenant Number', 'Meter Reading', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-', '.', 'E', 'A', 'B', 'C', 'D', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "first_class_index = 0\n",
    "assert(first_class_index in [0, 1])\n",
    "\n",
    "if first_class_index == 0:\n",
    "\n",
    "  # There is one class, balloon, with ID n°0.\n",
    "\n",
    "  num_classes = 41\n",
    "\n",
    "  finetuned_classes = ['Electricity Meter', 'Water Meter', 'Tenant Number', 'Meter Reading', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-', '.', 'E', 'A', 'B', 'C', 'D', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "  # The `no_object` class will be automatically reserved by DETR with ID equal\n",
    "  # to `num_classes`, so ID n°1 here.\n",
    "\n",
    "else:\n",
    "\n",
    "  # There is one class, balloon, with ID n°1.\n",
    "  #\n",
    "  # However, DETR assumes that indexing starts with 0, as in computer science,\n",
    "  # so there is a dummy class with ID n°0.\n",
    "  # Caveat: this dummy class is not the `no_object` class reserved by DETR.\n",
    "\n",
    "  num_classes = 42\n",
    "\n",
    "  finetuned_classes = ['Electricity Meter', 'Water Meter', 'Tenant Number', 'Meter Reading', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-', '.', 'E', 'A', 'B', 'C', 'D', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "  # The `no_object` class will be automatically reserved by DETR with ID equal\n",
    "  # to `num_classes`, so ID n°2 here.\n",
    "\n",
    "print('First class index: {}'.format(first_class_index))\n",
    "print('Parameter num_classes: {}'.format(num_classes))\n",
    "print('Fine-tuned classes: {}'.format(finetuned_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbcd4b1-665d-4a09-8685-54e1efae00e7",
   "metadata": {},
   "source": [
    "### Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5cf3447-6420-410b-b877-54dc4f17d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "from engine import evaluate, train_one_epoch\n",
    "from models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "111b0353-1ca1-4376-a37a-d162ec4ec024",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'lr': 1e-4,\n",
    "    'lr_backbone': 1e-5,\n",
    "    'batch_size': 2,\n",
    "    'weight_decay': 1e-4,\n",
    "    'epochs': 100,\n",
    "    'lr_drop': 200,\n",
    "    'clip_max_norm': 0.1,\n",
    "    # model parameters\n",
    "    'frozen_weights': None,\n",
    "    'backbone': 'resnet50',\n",
    "    'dilation': True, # store_true\n",
    "    'position_embedding': 'sine',\n",
    "    # Transformer\n",
    "    'enc_layers': 6,\n",
    "    'dec_layers': 6,\n",
    "    'dim_feedforward': 2048,\n",
    "    'hidden_dim': 256,\n",
    "    'dropout': 0.1,\n",
    "    'nheads': 8,\n",
    "    'num_queries': 100,\n",
    "    'pre_norm': True, # store_true\n",
    "    # Segmentation\n",
    "    'masks': False, # store_true\n",
    "    # loss\n",
    "    'aux_loss': True, # store_true\n",
    "    # Matcher\n",
    "    'set_cost_class': 1,\n",
    "    'set_cost_bbox': 5,\n",
    "    'set_cost_giou': 2,\n",
    "    # Loss coefficients\n",
    "    'mask_loss_coef': 1,\n",
    "    'dice_loss_coef': 1,\n",
    "    'bbox_loss_coef': 5,\n",
    "    'giou_loss_coef': 2,\n",
    "    'eos_coef': 0.1,\n",
    "    # dataset parameters\n",
    "    'num_classes': num_classes,\n",
    "    'dataset_file': 'coco', # need to add\n",
    "    'coco_path': './data/custom', # dataset path\n",
    "    'coco_panoptic_path': '',\n",
    "    'remove_difficult': True, # store_true\n",
    "    'output_dir': 'outputs', # output path\n",
    "    'device': 'cpu',\n",
    "    'seed': 42,\n",
    "    'resume': 'detr-r50_no-class-head.pth', # checkpoint path\n",
    "    'start_epoch': 0,\n",
    "    'eval': True,\n",
    "    'num_workers': 2,\n",
    "    # distributed training parameters\n",
    "    'world_size': 1,\n",
    "    'dist_url': 'env:://'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92dc3518-cb42-404b-b900-173f51024cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b9eba6-0a5c-421b-bcc4-a0b34e7d3f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "git:\n",
      "  sha: N/A, status: clean, branch: N/A\n",
      "\n",
      "Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=2, weight_decay=0.0001, epochs=100, lr_drop=200, clip_max_norm=0.1, frozen_weights=None, backbone='resnet50', dilation=True, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, pre_norm=True, masks=False, aux_loss=True, set_cost_class=1, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1, num_classes=41, dataset_file='coco', coco_path='./data/custom', coco_panoptic_path='', remove_difficult=True, output_dir='outputs', device='cpu', seed=42, resume='detr-r50_no-class-head.pth', start_epoch=0, eval=True, num_workers=2, world_size=1, dist_url='env:://', distributed=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /home)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    }
   ],
   "source": [
    "utils.init_distributed_mode(args)\n",
    "print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "    assert args.masks, \"Frozen training is meant for segmentation only\"\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d2fe793-e223-4d2d-bad7-ce670df2ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6c32a-1f93-4027-907e-b2d2ee792eb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, criterion, postprocessors = build_model(args)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dae6fabf-f9fe-48f6-b2a8-7a4ecc211f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 41290030\n"
     ]
    }
   ],
   "source": [
    "model_without_ddp = model\n",
    "if args.distributed:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "    model_without_ddp = model.module\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a00082c6-cc23-47a7-b09f-6826dad58aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.14s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "        \"lr\": args.lr_backbone,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                              weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "dataset_train = build_dataset(image_set='train', args=args)\n",
    "dataset_val = build_dataset(image_set='val', args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "550d7f4b-8c6c-436c-8982-4ac4d67d1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    sampler_train = DistributedSampler(dataset_train)\n",
    "    sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
    "else:\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                               collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                             drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "\n",
    "if args.dataset_file == \"coco_panoptic\":\n",
    "    # We also evaluate AP during panoptic training, on original coco DS\n",
    "    coco_val = datasets.coco.build(\"val\", args)\n",
    "    base_ds = get_coco_api_from_dataset(coco_val)\n",
    "else:\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e512410-0587-43b3-b1cb-82186616f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(args.output_dir)\n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "if args.eval:\n",
    "    test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
    "                                          data_loader_val, base_ds, device, args.output_dir)\n",
    "    if args.output_dir:\n",
    "        utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c50ba6-94cf-40cb-b952-fad13f2c9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    if args.distributed:\n",
    "        sampler_train.set_epoch(epoch)\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "        args.clip_max_norm)\n",
    "    lr_scheduler.step()\n",
    "    if args.output_dir:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "        # extra checkpoint before LR drop and every 100 epochs\n",
    "        if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 100 == 0:\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            utils.save_on_master({\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    test_stats, coco_evaluator = evaluate(\n",
    "        model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
    "    )\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                 **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                 'epoch': epoch,\n",
    "                 'n_parameters': n_parameters}\n",
    "\n",
    "    if args.output_dir and utils.is_main_process():\n",
    "        with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "        # for evaluation logs\n",
    "        if coco_evaluator is not None:\n",
    "            (output_dir / 'eval').mkdir(exist_ok=True)\n",
    "            if \"bbox\" in coco_evaluator.coco_eval:\n",
    "                filenames = ['latest.pth']\n",
    "                if epoch % 50 == 0:\n",
    "                    filenames.append(f'{epoch:03}.pth')\n",
    "                for name in filenames:\n",
    "                    torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
    "                               output_dir / \"eval\" / name)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9450a6f",
   "metadata": {},
   "source": [
    "### Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.plot_utils import plot_logs\n",
    "\n",
    "from pathlib import Path\n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "log_directory = [Path('outputs/')]\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc234ff-16ef-4e5c-bb40-fcaafc71a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logs(logs, fields=('class_error', 'loss_bbox_unscaled', 'mAP'), ewm_col=0, log_name='log.txt'):\n",
    "    '''\n",
    "    Function to plot specific fields from training log(s). Plots both training and test results.\n",
    "\n",
    "    :: Inputs - logs = list containing Path objects, each pointing to individual dir with a log file\n",
    "              - fields = which results to plot from each log file - plots both training and test for each field.\n",
    "              - ewm_col = optional, which column to use as the exponential weighted smoothing of the plots\n",
    "              - log_name = optional, name of log file if different than default 'log.txt'.\n",
    "\n",
    "    :: Outputs - matplotlib plots of results in fields, color coded for each log file.\n",
    "               - solid lines are training results, dashed lines are test results.\n",
    "\n",
    "    '''\n",
    "    func_name = \"plot_utils.py::plot_logs\"\n",
    "\n",
    "    # verify logs is a list of Paths (list[Paths]) or single Pathlib object Path,\n",
    "    # convert single Path to list to avoid 'not iterable' error\n",
    "\n",
    "    if not isinstance(logs, list):\n",
    "        if isinstance(logs, PurePath):\n",
    "            logs = [logs]\n",
    "            print(f\"{func_name} info: logs param expects a list argument, converted to list[Path].\")\n",
    "        else:\n",
    "            raise ValueError(f\"{func_name} - invalid argument for logs parameter.\\n \\\n",
    "            Expect list[Path] or single Path obj, received {type(logs)}\")\n",
    "\n",
    "    # Quality checks - verify valid dir(s), that every item in list is Path object, and that log_name exists in each dir\n",
    "    for i, dir in enumerate(logs):\n",
    "        if not isinstance(dir, PurePath):\n",
    "            raise ValueError(f\"{func_name} - non-Path object in logs argument of {type(dir)}: \\n{dir}\")\n",
    "        if not dir.exists():\n",
    "            raise ValueError(f\"{func_name} - invalid directory in logs argument:\\n{dir}\")\n",
    "        # verify log_name exists\n",
    "        fn = Path(dir / log_name)\n",
    "        if not fn.exists():\n",
    "            print(f\"-> missing {log_name}.  Have you gotten to Epoch 1 in training?\")\n",
    "            print(f\"--> full path of missing log file: {fn}\")\n",
    "            return\n",
    "\n",
    "    # load log file(s) and plot\n",
    "    dfs = [pd.read_json(Path(p) / log_name, lines=True) for p in logs]\n",
    "    # print(\"dfs >>> \", dfs)\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=len(fields), figsize=(16, 5))\n",
    "\n",
    "    for df, color in zip(dfs, sns.color_palette(n_colors=len(logs))):\n",
    "        for j, field in enumerate(fields):\n",
    "            if field == 'mAP':\n",
    "                coco_eval = pd.DataFrame(\n",
    "                    np.stack(df.test_coco_eval_bbox.dropna().values)[:, 1]\n",
    "                ).ewm(com=ewm_col).mean()\n",
    "                axs[j].plot(coco_eval, c=color)\n",
    "            else:\n",
    "                df.plot(\n",
    "                    y=[f'train_{field}', f'test_{field}'],\n",
    "                    ax=axs[j],\n",
    "                    color=[color] * 2,\n",
    "                    style=['-', '--']\n",
    "                )\n",
    "    for ax, field in zip(axs, fields):\n",
    "        ax.legend([Path(p).name for p in logs])\n",
    "        ax.set_title(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_of_interest = (\n",
    "    'loss',\n",
    "    'mAP',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012da7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_of_interest = (\n",
    "    'loss_ce',\n",
    "    'loss_bbox',\n",
    "    'loss_giou',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ebf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_of_interest = (\n",
    "    'class_error',\n",
    "    'cardinality_error_unscaled',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37e4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
